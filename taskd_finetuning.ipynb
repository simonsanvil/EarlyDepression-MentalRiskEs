{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fine-tune a pre-trained RoBERTa model from HuggingFace to perform multi-output regression on this task's data. We'll use `RobertaPreTrainedModel` from the `transformers` library to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification, \n",
    "    AdamW, BertConfig, Trainer, TrainingArguments,\n",
    "    RobertaForSequenceClassification, RobertaTokenizer,\n",
    "    RobertaPreTrainedModel, BertPreTrainedModel,\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src import data, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>round</th>\n",
       "      <th>date</th>\n",
       "      <th>message</th>\n",
       "      <th>d_suffer_in_favour</th>\n",
       "      <th>d_suffer_against</th>\n",
       "      <th>d_suffer_other</th>\n",
       "      <th>d_control</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject264</td>\n",
       "      <td>-1</td>\n",
       "      <td>2020-10-16 07:04:25</td>\n",
       "      <td>Alguien que quiera charlar ?</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  round                 date                       message  \\\n",
       "0  subject264     -1  2020-10-16 07:04:25  Alguien que quiera charlar ?   \n",
       "\n",
       "   d_suffer_in_favour  d_suffer_against  d_suffer_other  d_control  \n",
       "0                 0.3               0.4             0.3        0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape (augmented): (296, 8)\n",
      "val_data.shape: (27, 8)\n"
     ]
    }
   ],
   "source": [
    "# load the train data\n",
    "train_data = data.load('train').filter(regex='^(subject_id|round|date|message|d_)')\n",
    "display(train_data.head(1))\n",
    "# concat messages by subject id\n",
    "train_data = data.concat_messages(train_data)\n",
    "\n",
    "# split into 15% of subject ids for validation \n",
    "# get the classes as the argmax of the label probabilities to use them for stratification\n",
    "subj_classes = train_data.set_index('subject_id').filter(regex='^d_')\\\n",
    "    .apply(lambda x: x.argmax() if x[:-1].sum()<0.5 else x[:-1].argmax(), axis=1)\\\n",
    "        .replace(dict(enumerate(train_data.filter(regex='^d_').columns)))\n",
    "tr_subj_ids, val_subj_ids = train_test_split(subj_classes.index, test_size=0.15, random_state=42, stratify=subj_classes.values)\n",
    "# split the train data into train and validation sets\n",
    "df_val = train_data[train_data['subject_id'].isin(val_subj_ids)]\n",
    "df_train = train_data[train_data['subject_id'].isin(tr_subj_ids)]\n",
    "\n",
    "# augment the train data by taking only the first half of the messages\n",
    "half_messages_df_train = df_train.assign(\n",
    "    message=lambda df: df['message'].apply(lambda x: ' | '.join(x.split(' | ')[:len(x.split(' | '))//2])),\n",
    "    # num_messages=lambda df: df['message'].apply(lambda x: len(x.split(' | ')))\n",
    ")\n",
    "df_train = pd.concat([df_train, half_messages_df_train], axis=0).sort_values('subject_id').reset_index(drop=True)\n",
    "print(f\"train_df.shape (augmented): {df_train.shape}\")\n",
    "print(f\"val_data.shape: {df_val.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trained transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hackathon-somos-nlp-2023/roberta-base-bne-finetuned-suicide-es were not used when initializing RobertaPreTrainedModel: ['roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'classifier.out_proj.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'classifier.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'classifier.dense.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaPreTrainedModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaPreTrainedModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at hackathon-somos-nlp-2023/roberta-base-bne-finetuned-suicide-es were not used when initializing RobertaRegressor: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaRegressor from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaRegressor from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaRegressor were not initialized from the model checkpoint at hackathon-somos-nlp-2023/roberta-base-bne-finetuned-suicide-es and are newly initialized: ['roberta.pooler.dense.weight', 'classifier.weight', 'roberta.pooler.dense.bias', 'regressor.bias', 'regressor.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoTokenizer, RobertaPreTrainedModel\n",
    "from src.roberta_regressor import RobertaRegressor, multi_reg_loss, train, evaluate\n",
    "\n",
    "model_name = \"PlanTL-GOB-ES/roberta-base-bne\"\n",
    "model_name = 'hackathon-somos-nlp-2023/roberta-base-bne-finetuned-suicide-es'\n",
    "model = RobertaPreTrainedModel.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = RobertaRegressor.from_pretrained(model_name, num_outputs=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MentalRiskDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer, max_len: int = 1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "        text = data_row.message\n",
    "        label = torch.tensor(data_row.label)\n",
    "        tokens = self.tokenizer.tokenize(text, padding=True, truncation=True)\n",
    "        if len(tokens) > self.max_len - 2:\n",
    "            tokens = tokens[:self.max_len - 2]\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "        if len(tokens) < self.max_len:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.max_len - len(tokens))] \n",
    "        else:\n",
    "            tokens = tokens[:self.max_len-1] + ['[SEP]'] \n",
    "        input_ids = torch.tensor(self.tokenizer.convert_tokens_to_ids(tokens)).squeeze(0) \n",
    "        # pooling layer\n",
    "        \n",
    "        attention_mask = (input_ids != 0).long()\n",
    "        return input_ids, attention_mask, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject104</th>\n",
       "      <td>Sin duda alguna , mal | Algo general que puedo...</td>\n",
       "      <td>[0.4, 0.0, 0.3, 0.3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject104</th>\n",
       "      <td>Sin duda alguna , mal | Algo general que puedo...</td>\n",
       "      <td>[0.4, 0.0, 0.3, 0.3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject108</th>\n",
       "      <td>Ocupado con la uni | Unmsm es universidad naci...</td>\n",
       "      <td>[0.4, 0.1, 0.0, 0.5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      message  \\\n",
       "subject_id                                                      \n",
       "subject104  Sin duda alguna , mal | Algo general que puedo...   \n",
       "subject104  Sin duda alguna , mal | Algo general que puedo...   \n",
       "subject108  Ocupado con la uni | Unmsm es universidad naci...   \n",
       "\n",
       "                           label  \n",
       "subject_id                        \n",
       "subject104  [0.4, 0.0, 0.3, 0.3]  \n",
       "subject104  [0.4, 0.0, 0.3, 0.3]  \n",
       "subject108  [0.4, 0.1, 0.0, 0.5]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject101</th>\n",
       "      <td>volvi me extrañaron signo de interrogación | y...</td>\n",
       "      <td>[0.7, 0.1, 0.1, 0.1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject106</th>\n",
       "      <td>Nunca me imagine terminar tan mal | Y necesita...</td>\n",
       "      <td>[0.5, 0.5, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject11</th>\n",
       "      <td>Y al día de hoy , cómo sigues después de todo ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      message  \\\n",
       "subject_id                                                      \n",
       "subject101  volvi me extrañaron signo de interrogación | y...   \n",
       "subject106  Nunca me imagine terminar tan mal | Y necesita...   \n",
       "subject11   Y al día de hoy , cómo sigues después de todo ...   \n",
       "\n",
       "                           label  \n",
       "subject_id                        \n",
       "subject101  [0.7, 0.1, 0.1, 0.1]  \n",
       "subject106  [0.5, 0.5, 0.0, 0.0]  \n",
       "subject11   [0.0, 0.0, 0.0, 1.0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape: (296, 2)\n",
      "val_df.shape: (27, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df = df_train.set_index('subject_id').assign(\n",
    "    label=lambda df: df.filter(regex='^d_').values.tolist()\n",
    ")[['message', 'label']]\n",
    "val_df = df_val.set_index('subject_id').assign(\n",
    "    label=lambda df: df.filter(regex='^d_').values.tolist()\n",
    ")[['message', 'label']]\n",
    "\n",
    "display(train_df.head(3))\n",
    "display(val_df.head(3))\n",
    "print(f\"train_df.shape: {train_df.shape}\")\n",
    "print(f\"val_df.shape: {val_df.shape}\")\n",
    "\n",
    "MAX_LEN = 512\n",
    "\n",
    "train_dataset = MentalRiskDataset(train_df, tokenizer, max_len=MAX_LEN)\n",
    "# test_dataset = MentalRiskDataset(test_df, tokenizer)\n",
    "val_dataset = MentalRiskDataset(val_df.sample(frac=0.3), tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d_control             69\n",
       "d_suffer_against      37\n",
       "d_suffer_in_favour    37\n",
       "d_suffer_other         5\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "d_control             12\n",
       "d_suffer_against       7\n",
       "d_suffer_in_favour     7\n",
       "d_suffer_other         1\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(subj_classes[tr_subj_ids].value_counts())\n",
    "display(subj_classes[val_subj_ids].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb # to log the training progress\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"mentalriskes-roberta-finetuning\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"init_learning_rate\": 3e-5,\n",
    "    \"architecture\": \"BERT\",\n",
    "    \"dataset\": \"MENTALRISKES\"}\n",
    ")\n",
    "def evaluate(model, criterion, dataloader, device):\n",
    "    \"\"\"\n",
    "    To evaluate the model on the validation set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    mean_acc, mean_loss, count = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, target in (dataloader):\n",
    "            \n",
    "            input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n",
    "            output = model(input_ids, attention_mask)\n",
    "            mean_loss += criterion(output.squeeze(), target.type_as(output).squeeze()).item()\n",
    "            count += 1\n",
    "            \n",
    "    return mean_loss/count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf-env/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "criterion = CrossEntropyLoss() #multi_reg_loss(\"cross_entropy\",sum_diff_penalty=0.1)\n",
    "optimizer = AdamW(params=model.parameters(), lr=3e-5)\n",
    "device = \"mps\"\n",
    "\n",
    "num_epochs = 20\n",
    "unfreeze_at = 12 # epoch\n",
    "unfreeze_percent = 0.1 # 0.1 = 10% of the (last) layers will be unfrozen at each epoch after unfreeze_at\n",
    "unfrozen_count = 0\n",
    "loss_per_epoch = []\n",
    "\n",
    "num_training_steps=len(train_loader) * num_epochs\n",
    "scheduler = transformers.get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "roberta_params = list(model.roberta.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "first_epoch = 0\n",
    "epochs = num_epochs\n",
    "log_every = 3\n",
    "save_every = 10\n",
    "model = model.to(device)\n",
    "for epoch in trange(first_epoch, epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, (input_ids, attention_mask, target) in enumerate(iterable=train_loader):\n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n",
    "        \n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(output.squeeze(), target.type_as(output).squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "    if unfreeze_at and epoch>=unfreeze_at:\n",
    "      print(f\"Unfreezing params at epoch {epoch}\")\n",
    "      # unfreeze the roberta parameters\n",
    "      if unfrozen_count < len(roberta_params):\n",
    "        unfrozen_count += int(len(roberta_params) * unfreeze_percent)\n",
    "        for param in roberta_params[-unfrozen_count:]:\n",
    "          param.requires_grad = True\n",
    "    if (epoch%log_every == 0 or (epoch==epochs-1)):\n",
    "      print(f\"Training loss is {train_loss/len(train_loader)}\")\n",
    "      val_loss = evaluate(model=model, criterion=criterion, dataloader=val_loader, device=device)\n",
    "      print(\"Epoch {} complete! Validation Loss : {}\".format(epoch+1, val_loss))\n",
    "      loss_dict = {\n",
    "        \"epoch\":epoch,\n",
    "        \"train\":train_loss/len(train_loader), \n",
    "        \"validation\":val_loss,\n",
    "        \"lr\":optimizer.param_groups[0]['lr']\n",
    "      }\n",
    "      loss_per_epoch.append(loss_dict)\n",
    "      wandb.log(loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "loss_df = pd.DataFrame(loss_per_epoch).assign(epoch=lambda df: df.epoch + 1)\n",
    "ax = loss_df.set_index('epoch').plot()\n",
    "ax.grid(); ax.set(ylabel=\"Loss\");\n",
    "ax.set_xticks(loss_df.epoch.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), \"models/roberta-finetuned.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model = pickle.load(open(\"models/roberta-base-bne-finetuned-25.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "# model = model.cpu()\n",
    "device=\"cpu\"\n",
    "\n",
    "def normalize(x, weights:Tuple[float,float,float,float] = None):\n",
    "    if weights is None:\n",
    "        weights = (0.25,0.25,0.25,0.25)\n",
    "    # normalize to [0,1]\n",
    "    x = (x - x.min())/(x.max() - x.min())\n",
    "    # normalize to [-1,1]\n",
    "    x = (x - 0.5)*2\n",
    "    # apply weights\n",
    "    x = x * np.array(weights)\n",
    "    x /= x.sum()\n",
    "    return x\n",
    "\n",
    "def predict(text:str, model, tokenizer, device) -> Tuple[float, float, float, float]:\n",
    "    input_ids, attention_mask = tokenizer.encode_plus(text, padding=True, truncation=True, return_tensors='pt').values()\n",
    "    input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "    output = model(input_ids, attention_mask).squeeze()\n",
    "    # free up memory\n",
    "    del input_ids, attention_mask\n",
    "    return output.detach().numpy()\n",
    "\n",
    "def label_metrics(score_fun, y_true, y_pred):\n",
    "    scores = []\n",
    "    for i in range(len(label_names)):\n",
    "        scores.append(score_fun(y_true[:,i],y_pred[:,i]))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Es estudiar algo o me echan de casa para cosa ...</td>\n",
       "      <td>[0.5, 0.2, 0.2, 0.1]</td>\n",
       "      <td>[-0.7, 0.02, -1.52, 0.83]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Si es una ayuda momentánea , espero que con má...</td>\n",
       "      <td>[0.5, 0.2, 0.0, 0.3]</td>\n",
       "      <td>[-0.7, 0.02, -1.52, 0.83]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Tu paes nose pueden Meter en tu vida sentiment...</td>\n",
       "      <td>[0.8, 0.0, 0.1, 0.1]</td>\n",
       "      <td>[-0.7, 0.02, -1.52, 0.83]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Estas muy linda cara feliz con ojos sonrientes...</td>\n",
       "      <td>[0.1, 0.0, 0.1, 0.8]</td>\n",
       "      <td>[-0.7, 0.02, -1.52, 0.83]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>La verdad es que para mí es un castigo la comi...</td>\n",
       "      <td>[0.1, 0.1, 0.2, 0.6]</td>\n",
       "      <td>[-0.7, 0.02, -1.52, 0.83]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               message                 label  \\\n",
       "86   Es estudiar algo o me echan de casa para cosa ...  [0.5, 0.2, 0.2, 0.1]   \n",
       "0    Si es una ayuda momentánea , espero que con má...  [0.5, 0.2, 0.0, 0.3]   \n",
       "99   Tu paes nose pueden Meter en tu vida sentiment...  [0.8, 0.0, 0.1, 0.1]   \n",
       "141  Estas muy linda cara feliz con ojos sonrientes...  [0.1, 0.0, 0.1, 0.8]   \n",
       "312  La verdad es que para mí es un castigo la comi...  [0.1, 0.1, 0.2, 0.6]   \n",
       "\n",
       "                     predicted  \n",
       "86   [-0.7, 0.02, -1.52, 0.83]  \n",
       "0    [-0.7, 0.02, -1.52, 0.83]  \n",
       "99   [-0.7, 0.02, -1.52, 0.83]  \n",
       "141  [-0.7, 0.02, -1.52, 0.83]  \n",
       "312  [-0.7, 0.02, -1.52, 0.83]  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df_sampled = val_df.sample(60)\n",
    "val_predictions = val_df_sampled.message.apply(lambda x: predict(x, model, tokenizer, device=device))\n",
    "results_df = val_df_sampled.assign(\n",
    "    predicted=val_predictions.apply(lambda x: x.round(2)).tolist()\n",
    ")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score is -0.4116190829023336\n",
      "RMSE score is 0.2525223318253405\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "r2 = r2_score(val_df_sampled.label.tolist(), val_predictions.apply(list).tolist())\n",
    "rmse = mean_squared_error(val_df_sampled.label.tolist(), val_predictions.apply(list).tolist(), squared=False)\n",
    "print(f\"R2 score is {r2}\")\n",
    "print(f\"RMSE score is {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>suffer_in_favour</th>\n",
       "      <td>0.275965</td>\n",
       "      <td>0.128641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suffer_against</th>\n",
       "      <td>0.257571</td>\n",
       "      <td>-0.290715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suffer_other</th>\n",
       "      <td>0.126232</td>\n",
       "      <td>-1.655737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>control</th>\n",
       "      <td>0.350322</td>\n",
       "      <td>0.171335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label                 rmse        r2\n",
       "suffer_in_favour  0.275965  0.128641\n",
       "suffer_against    0.257571 -0.290715\n",
       "suffer_other      0.126232 -1.655737\n",
       "control           0.350322  0.171335"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "true = np.array(val_df_sampled.label.apply(list).tolist())\n",
    "pred = np.array(val_predictions.apply(list).tolist())\n",
    "rmse_metrics = label_metrics(\n",
    "    partial(mean_squared_error, squared=False), \n",
    "    true, pred\n",
    ")\n",
    "rmse_metrics = dict(zip(label_names, rmse_metrics))\n",
    "r2_metrics = dict(zip(label_names, label_metrics(r2_score, true, pred)))\n",
    "metrics_df = pd.DataFrame([rmse_metrics, r2_metrics], index=['rmse', 'r2']).T.rename_axis('label', axis=1)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16364938, 0.23866598, 0.04136196, 0.5285845 ], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# inference example\n",
    "text = \"como puedo ayudarte?\"\n",
    "input_ids, attention_mask = tokenizer.encode_plus(text, padding=True, truncation=True, return_tensors='pt').values()\n",
    "out = model(input_ids, attention_mask).squeeze().detach().numpy()\n",
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
